{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83413848",
   "metadata": {},
   "source": [
    "# Task-2: Binary Classification of Sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f6134f",
   "metadata": {},
   "source": [
    "### Importing Necessary Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4810432a",
   "metadata": {},
   "source": [
    "I will be using Spacy for tokenization and word embeddings. TensorFlow framework is used to build the Binary Classification Model. Scikit-Learn provides us the metrics for the evaluation purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "515e2fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import spacy\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1b9e9d",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c8be48",
   "metadata": {},
   "source": [
    "The model is trained on the following very small amount of fabricated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fa4d029",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    'you won a billion dollars , great work !',\n",
    "    'click here for cs685 midterm answers',\n",
    "    'read important cs685 news',\n",
    "    'send me your bank account info asap'\n",
    "]\n",
    "labels = [1, 1, 0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38b4d50",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c3b9a4",
   "metadata": {},
   "source": [
    "The **en_core_web_sm** model provided by Spacy is used to calculate the word embedding vector for each word. Now, one issue with this **en_core_web_sm**, i.e. the small model, is it doesn't comes with static word vectors or vocabulary. Hence, there was no way to form an Embedding Matrix which requires vocabulary size and token ids. For this purpose, the pre-processing to convert the tokens to vectors is done before inputing in our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8919ee34",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\") # Loading Spacy Model\n",
    "EMBED_DIM = len(nlp(\"Hi\").vector) # Spacy's Embedding Dimension Size\n",
    "MAX_LEN = 10 # The max length of the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4059c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = nlp.tokenizer\n",
    "# Tokenizing and embedding the words using the Spacy Model\n",
    "train_data = [[nlp(str(word)).vector for word in list(tokenizer(sent))] for sent in sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a94a5d",
   "metadata": {},
   "source": [
    "Since, the padding and word to vector conversion is done before feeding into the model. It was easier to generate the corresponding mask for the padding tokens during this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d016136",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = list()\n",
    "for sent in train_data:\n",
    "    k = len(sent)\n",
    "    sent_mask = [1 for _ in range(k)] # These positions have a word, so mask value is 1\n",
    "    for _ in range(MAX_LEN - k):\n",
    "        sent.append(np.zeros(EMBED_DIM)) # The padding positions have all 0s embdedding vectors for casting into matrix\n",
    "        sent_mask.append(0) # The rest of the positions doesn't have a word hence are padding, so mask value is 0\n",
    "    mask.append(sent_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bef9289b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1 Max\n",
      "Train Data Shape: (4, 10, 96)\n",
      "Padding Mask Shape: (4, 1, 1, 10)\n",
      "Labels Shape: (4,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-01 18:42:16.051888: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-09-01 18:42:16.052218: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "train_data = tf.cast(np.array(train_data), dtype=tf.float32) # Embedded training data\n",
    "mask = tf.cast(np.array(mask), dtype=tf.float32)[:, tf.newaxis, tf.newaxis, :] # Padding Mask\n",
    "labels = tf.cast(np.array(labels), dtype=tf.float32) # Labels\n",
    "print(\"Train Data Shape:\", train_data.shape)\n",
    "print(\"Padding Mask Shape:\", mask.shape)\n",
    "print(\"Labels Shape:\", labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87c1bb8",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a215a7",
   "metadata": {},
   "source": [
    "The following are the model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85daf3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "POS_ENC_ANGLE_DENO = 10000 # Denominator angle in Positional Encoding\n",
    "NUM_ENC_LAYERS =  2 # Number of Encoder Blocks or Layers\n",
    "NUM_HEADS =  2 # Number of heads\n",
    "EMBED_DIM = 96 # Embedding Dimension\n",
    "FEED_FORWARD_DIM = 32 # Feed Forward NNs number of units in hidden layer\n",
    "DROPOUT_RATE = 0.1 # Dropout Rate\n",
    "MAX_LEN = 10 # Max length of each tokenized sentence\n",
    "BATCH_SIZE = 4 # Training Batch Size\n",
    "EPOCHS = 10 # Number of epochs to train model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e34838",
   "metadata": {},
   "source": [
    "We calculate the positional encoding for each word vector using,\n",
    "$$PE_{(pos, 2i)} = \\sin(pos/10000^{2i/d_{model}})$$\n",
    "$$PE_{(pos, 2i+1)} = \\cos(pos/10000^{2i/d_{model}})$$\n",
    "where, $2i$, $2i+1$ is the index in the column of the word embedding vector and $pos$ is the position of the word in the padded tokenized sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b3af46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_enc(max_len, d_model):\n",
    "    # returns the positional encoding matrix which needs to be added to the embedding matrix\n",
    "    angles = np.arange(max_len)[:, np.newaxis] / np.power(POS_ENC_ANGLE_DENO, 2*(np.arange(d_model)[np.newaxis, :]//2/np.float32(d_model)))\n",
    "    pos_encode = np.zeros((max_len, d_model))\n",
    "    pos_encode[:, 0::2] = np.sin(angles[:, 0::2])\n",
    "    pos_encode[:, 1::2] = np.cos(angles[:, 1::2])\n",
    "    return tf.cast(pos_encode[np.newaxis, :], dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3fda82",
   "metadata": {},
   "source": [
    "We calculate the padding mask which is basically the same as the original padding mask just that it has $2$ extra dimensions to make up for the dimension corresponding to the heads and embedding vector length in the heads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8773807",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_mask(mat):\n",
    "    # recasts the padding mask with little different dimensions\n",
    "    mask = tf.cast(tf.math.equal(mat, 0), dtype=tf.float32)\n",
    "    return mask[:, tf.newaxis, tf.newaxis, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20c9800",
   "metadata": {},
   "source": [
    "We compute the attention score of each word with the other word using the query vector, key vector and value vector for each word which when stacked forms the $Q$, $K$ and $V$ matrices. The attention vector (stacked to form attention matrix) is calculated using the scaled dot product attention formula given by,\n",
    "$$Attention(Q,K,V) = softmax_k(\\frac{QK^T}{\\sqrt{d_k}})V$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "380bf3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_prod_attn(q, k, v, mask):\n",
    "    # returns the scaled dot product attention based on queries, keys and values and mask\n",
    "    qk = tf.matmul(q, k, transpose_b=True) # calculates the numerator of the softmax input\n",
    "    dk = tf.cast(tf.shape(k)[-1], dtype=tf.float32)\n",
    "    pre_softmax = qk / tf.sqrt(dk) # calculates the angle input into softmax\n",
    "    if mask is not None:\n",
    "        pre_softmax += (mask * 1e-9) # padding mask as softmax would give almost zero for these positions\n",
    "    attn_wts = tf.nn.softmax(pre_softmax, axis=-1) # attention weights per word for other words\n",
    "    final_attention = tf.matmul(attn_wts, v) # value vectors weighted average with attention weights\n",
    "    return final_attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e827fd9",
   "metadata": {},
   "source": [
    "Now, the Q, K and V matrix are linearly transformed to learn the nature of queries, keys and values across various representations in different subspace. For each of these representations the attentions are calculated as mentioned above in the different heads and these are then stacked together to be passed to the next layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "308ea081",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MHA(tf.keras.layers.Layer):\n",
    "    # class for multi-head attention\n",
    "    def __init__(self, *, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        assert d_model % num_heads == 0 # checks if split of d_model is position among heads\n",
    "        self.d_head = self.d_model // self.num_heads\n",
    "        \n",
    "        self.linear_q = tf.keras.layers.Dense(self.d_model)\n",
    "        self.linear_k = tf.keras.layers.Dense(self.d_model)\n",
    "        self.linear_v = tf.keras.layers.Dense(self.d_model)\n",
    "        \n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "    def div_heads(self, dat, batch_size):\n",
    "        dat = tf.reshape(dat, (batch_size, -1, self.num_heads, self.d_head))\n",
    "        return tf.transpose(dat, perm=[0, 2, 1, 3])\n",
    "    \n",
    "    def call(self, V, K, Q, mask):\n",
    "        batch_size = tf.shape(Q)[0]\n",
    "        \n",
    "        # Linear Transformation for different representation in different heads\n",
    "        Q = self.linear_q(Q)\n",
    "        K = self.linear_k(K)\n",
    "        V = self.linear_v(V)\n",
    "        \n",
    "        # split K, Q, V matrix among heads\n",
    "        q = self.div_heads(Q, batch_size)\n",
    "        k = self.div_heads(K, batch_size)\n",
    "        v = self.div_heads(V, batch_size)\n",
    "        \n",
    "        # calculate the scaled dot product in each head\n",
    "        scaled_attn = scaled_dot_prod_attn(q, k, v, mask)\n",
    "        scaled_attn = tf.transpose(scaled_attn, perm=[0, 2, 1, 3])\n",
    "        # concatenate the attention vectors from each head\n",
    "        concat_attn = tf.reshape(scaled_attn, (batch_size, -1, self.d_model)) \n",
    "        \n",
    "        final_output = self.dense(concat_attn)\n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25db2058",
   "metadata": {},
   "source": [
    "After the attentions are being calculated, we need some non-linearity in our model as uptil now we had no source of non-linearity in our model. That is where we pass these attention vectors generated from MHA is passed through the Feedforward Neural Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a80d73db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_MHA_FF_Net(d_model, d_ff):\n",
    "    # returns the post MHA feed forward neural network\n",
    "    FF_Net = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(d_ff, activation='relu'),\n",
    "        tf.keras.layers.Dense(d_model)\n",
    "    ])\n",
    "    return FF_Net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd9f740",
   "metadata": {},
   "source": [
    "Now, the above computation i.e. Multi-Headed Attention followed by Feed Forward Neural Network (with skip connections across both MHA and Feed Forward part) forms a single layer/block of the Encoder part of a Transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4bc00390",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    # returns the model where MHA, skip connection, batch norm and feed forward network, skip connection and batch norm is added\n",
    "    def __init__(self, *, d_model, num_heads, num_nodes, drop_rate=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.mha = MHA(d_model=d_model, num_heads=num_heads)\n",
    "        self.ffn = post_MHA_FF_Net(d_model, num_nodes)\n",
    "        \n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "        self.dropout1 = tf.keras.layers.Dropout(drop_rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(drop_rate)\n",
    "        \n",
    "    def call(self, x, train, mask):\n",
    "        # calculating the multi head attention\n",
    "        attn_output = self.mha(x, x, x, mask)\n",
    "        attn_output = self.dropout1(attn_output, training=train)\n",
    "        output1 = self.layernorm1(x + attn_output) # skip connection\n",
    "        \n",
    "        # feeding the concatenated attention matrix output of MHA into Feed-Forward Network\n",
    "        ffn_output = self.ffn(output1)\n",
    "        ffn_output = self.dropout2(ffn_output, training = train)\n",
    "        final_output = self.layernorm2(x + ffn_output) # skip connection\n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e41185d",
   "metadata": {},
   "source": [
    "We repeat the Encoder Block twice, as decided in our parameter after the positional encoding is added to build the final Encoder part of the Transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1fc4b78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    # returns model which takes the embeddings as input along with mask and adds positional encoding \n",
    "    # and 2 encoder blocks\n",
    "    def __init__(self, *, num_layers, d_model, num_heads, num_nodes, drop_rate=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "        self.pos_encoding = pos_enc(MAX_LEN, self.d_model)\n",
    "        \n",
    "        self.enc_layers = [\n",
    "            EncoderLayer(d_model=d_model, num_heads=num_heads, num_nodes=num_nodes, drop_rate=drop_rate)\n",
    "            for _ in range(num_layers)\n",
    "        ]\n",
    "        \n",
    "        self.dropout = tf.keras.layers.Dropout(drop_rate)\n",
    "        \n",
    "    def call(self, x, mask, train=True):\n",
    "        sent_len = tf.shape(x)[1]\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, dtype=tf.float32))\n",
    "        x += self.pos_encoding[:, :sent_len, :] # adding positional encoding\n",
    "        \n",
    "        x = self.dropout(x, training=train)\n",
    "        # passing the embedded and position encoded data through 1st encoder layer and the output is passed through\n",
    "        # another encoder layer\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, train, mask)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995070dc",
   "metadata": {},
   "source": [
    "Now, as mentioned earlier the model inputs are going to be the pre-processed train data and the padding mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3198a7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "input1 = tf.keras.layers.Input(shape=(MAX_LEN, EMBED_DIM)) # Word Embedding Matrix input with padding upto MAX_LEN\n",
    "input2 = tf.keras.layers.Input(shape=(1, 1, MAX_LEN)) # Padding Mask Matrix \n",
    "transformer = Encoder(\n",
    "    num_layers=NUM_ENC_LAYERS, \n",
    "    d_model=EMBED_DIM, \n",
    "    num_heads=NUM_HEADS, \n",
    "    num_nodes=FEED_FORWARD_DIM, \n",
    "    drop_rate=DROPOUT_RATE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685d1a8c",
   "metadata": {},
   "source": [
    "The Encoder block learns a feature vector for each word which assists the classification. For classification we average across the temporal axis of the feature vector of the words. This concise vector is now passed through another feed forward neural network with dropouts and finally is hooked up with a $2$ neuron output layer with softmax activation. Where the first neuron corresponds to the predicted probability of the sentence belonging to the label $0$ and the second neuron corresponds to that for the label $1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4322af5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For classification we take the representation from the transformer and feeds them into the following feed forward\n",
    "# network for classification\n",
    "x = transformer(input1, input2, True)\n",
    "x = tf.keras.layers.GlobalAveragePooling1D()(x) # averaging over the temporal axis of the resultant vector from transformer\n",
    "x = tf.keras.layers.Dropout(DROPOUT_RATE)(x)\n",
    "x = tf.keras.layers.Dense(FEED_FORWARD_DIM, activation='relu')(x) # Feed Forward Network for classification\n",
    "x = tf.keras.layers.Dropout(DROPOUT_RATE)(x)\n",
    "output = tf.keras.layers.Dense(2, activation='softmax')(x) # 0th unit corresponds to label 0 and 1st unit to label 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "90b8198c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Model(inputs=[input1, input2], outputs=output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a426a9",
   "metadata": {},
   "source": [
    "The following summarizes our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dfb179b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 10, 96)]     0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 1, 1, 10)]   0           []                               \n",
      "                                                                                                  \n",
      " encoder (Encoder)              (None, 10, 96)       87808       ['input_1[0][0]',                \n",
      "                                                                  'input_2[0][0]']                \n",
      "                                                                                                  \n",
      " global_average_pooling1d (Glob  (None, 96)          0           ['encoder[0][0]']                \n",
      " alAveragePooling1D)                                                                              \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 96)           0           ['global_average_pooling1d[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " dense_12 (Dense)               (None, 32)           3104        ['dropout_5[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)            (None, 32)           0           ['dense_12[0][0]']               \n",
      "                                                                                                  \n",
      " dense_13 (Dense)               (None, 2)            66          ['dropout_6[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 90,978\n",
      "Trainable params: 90,978\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7d73b6",
   "metadata": {},
   "source": [
    "The model is trained using the Adam Optimizer and the loss function chosen is the sparse categorical crossentropy as the true labels are scaler numbers. Batch Size of $4$ was used and the model was trained for $10$ epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7c4c2f2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-01 18:42:16.611635: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2022-09-01 18:42:17.328229: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step - loss: 0.9831 - accuracy: 0.2500\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.6221 - accuracy: 0.5000\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.3784 - accuracy: 0.7500\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.2549 - accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.2227 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.1430 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.1006 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0884 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.1385 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0560 - accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Training Model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "history = model.fit(\n",
    "    [train_data, mask], labels, batch_size=BATCH_SIZE, epochs=EPOCHS \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d540a24f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-01 18:42:18.405496: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 504ms/step\n"
     ]
    }
   ],
   "source": [
    "pred_class_probs = model.predict([train_data, mask])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c632ea",
   "metadata": {},
   "source": [
    "We take the index of the maximum of the two predicted probabilities for each sentence to be the predicted label of that sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "abc8b243",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_labels = np.argmax(pred_class_probs, axis=-1) # Model output is maximum of the two probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d35c482",
   "metadata": {},
   "source": [
    "It can be seen that the model predicts with accuracy $1$ all the labels. But ofcourse our model is overfit due to very small training data and the model performance on train data only is unreliable estimate of the actual model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ee04e91e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 1])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c321ec",
   "metadata": {},
   "source": [
    "The precision score, recall score and the F1 score is reported below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a889313c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Model Performance ***\n",
      "Precision Score: 1.0\n",
      "Recall Score: 1.0\n",
      "F1 Score: 1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"*** Model Performance ***\")\n",
    "print(\"Precision Score:\", precision_score(labels, pred_labels))\n",
    "print(\"Recall Score:\", recall_score(labels, pred_labels))\n",
    "print(\"F1 Score:\", f1_score(labels, pred_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42c853f",
   "metadata": {},
   "source": [
    "Writing the results in a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e431cdab",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"TensorFlow_results.csv\"\n",
    "fields = ['precision', 'recall', 'F1'] \n",
    "results = [precision_score(labels, pred_labels), recall_score(labels, pred_labels), f1_score(labels, pred_labels)]\n",
    "with open(filename, 'w') as csvfile: \n",
    "    csvwriter = csv.writer(csvfile) \n",
    "    csvwriter.writerow(fields)\n",
    "    csvwriter.writerow(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
