{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "065411ee",
   "metadata": {},
   "source": [
    "# Task-1.1:  Sentence Negation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49dde3a8",
   "metadata": {},
   "source": [
    "### Importing Necessary Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b75f52b",
   "metadata": {},
   "source": [
    "NLTK library is used for the purpose of tokenization and the Hugging Face's datasets library provides us with the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a855dcf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import string\n",
    "from datasets import load_dataset\n",
    "import nltk\n",
    "from nltk import word_tokenize, pos_tag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a130a41d",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e961b088",
   "metadata": {},
   "source": [
    "The Multi-Genre Natural Language Inference (MultiNLI) corpus is a crowd-sourced collection of 433k sentence pairs annotated with textual entailment information. We load this library available through the datasets library below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "28092dbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset glue (/Users/rishideychowdhury/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.009190797805786133,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 24,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 5,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbeeaaeab46b40b48a54f900f0fb6fae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset('glue', 'mnli')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a41c38",
   "metadata": {},
   "source": [
    "The first $5000$ sentences are chosen for the purpose of this negation task and will be used further in the second task for training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "03263d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataset['train']['premise'][:5000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea99e7f",
   "metadata": {},
   "source": [
    "### Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb9f953",
   "metadata": {},
   "source": [
    "For any NLP workflow, tokenization is the most basic step towards handling text data and below the word_tokenizer is used for this task provided by the NLTK library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0d3cb648",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tokenized = [word_tokenize(sent) for sent in data]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4ea3c2",
   "metadata": {},
   "source": [
    "Since, our purpose is the negate the English sentences using 'not' negation. We need to know few rules about how to do so.\n",
    "- Insert the negation marker ‘not’ after the first auxiliary verb in the original true target sentence to generate a new distractor sentence. \n",
    "- If the sentence already contains the negation marker ‘not’, we instead remove it. \n",
    "\n",
    "The following is the reason why this sort of data processing is of particular interest:\n",
    "- ‘Not’ Negation aims to detect whether a sentence embedding model is misled by the negation of a sentence relation caused by adding the word ‘not’."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1651f364",
   "metadata": {},
   "source": [
    "So, the 'not' negation task boils down to finding the first auxiliary verb of a sentence. Since, in English the 'not' word appears just after the auxiliary verb i.e. the form a 'not' negated simple English sentence is:\n",
    "$$S\\ +\\ AV\\ +\\ not\\ +\\ MV\\ +\\ R$$\n",
    "where, $S$ is subject, $AV$ is auxiliary verb, $MV$ is main verb and $R$ is rest of the sentence.\n",
    "\n",
    "Similarly, most of the English sentences with 'not' appears in the above form and we can simply remove the 'not' word to non-negate the sentece.\n",
    "\n",
    "In English, there are only a finitely many auxiliary verb as mentioned in the cell below, which are allowed to take 'not' word after it and we leverage this property for our purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "80684361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auxiliary Verbs\n",
    "aux_verbs = [\n",
    "    'is', 'am', 'are', 'was', 'were', 'do', 'does', 'did', 'have', 'has', 'had', 'may', 'can', 'will', 'shall', \n",
    "    'might', 'could', 'would', 'should', 'must', 'ought', 'need', 'dare', 'used', \"'m\", \"'re\", \"'ve\", \"'ll\", \"'d\", \"'s\" \n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81b9bbd",
   "metadata": {},
   "source": [
    "Some practical details kept in mind while implementing the sentence negation function:\n",
    "- One thing to observe is there are contracted words like 'm, 're, 've, 'll, 'd and n't. Special care is taken to deal with these words as 's is also a possesive pronoun and 'not' should not be added after it. Hence, we use the part of speech tagger to identify these tokens and check if they are verb or not.\n",
    "- The original sentence structure is preserved to the maximum extent as possible i.e. no extra spaces or punctuation marks or words are added, except for the 'not' word insertion or deletion.\n",
    "- **If a sentence is joined by using a coordinating or subordinating conjunction then both the sentences are negated.** The conjunctions are identifies using the POS tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "80a0dd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stores the 'not' negated sentences and the 'not' word removed sentences\n",
    "data_negated = list()\n",
    "for i in range(len(data)):\n",
    "    neg_sent_tokens = list() # Store the tokens of the new negated sentence\n",
    "    neg_flag = 1 # flag to remove only the first not word or add the first not word\n",
    "    sent_pos_tagged = pos_tag(data_tokenized[i]) # POS tagging sentence to identify conjunctions, possesive pronouns, etc\n",
    "    \n",
    "    for j in range(len(data_tokenized[i])):\n",
    "        if neg_flag == 1 and (data_tokenized[i][j].lower() in ['not', \"n't\"]) and j > 0:\n",
    "            if data_tokenized[i][j-1].lower() in aux_verbs:\n",
    "                # If current token is among variations of 'not' coming after a auxiliary verb then skip it \n",
    "                neg_flag = 0\n",
    "                continue\n",
    "            neg_sent_tokens.append(data_tokenized[i][j])\n",
    "        elif neg_flag == 1 and data_tokenized[i][j].lower() in aux_verbs and ('V' in sent_pos_tagged[j][1] or 'MD' in sent_pos_tagged[j][1]) and  j < len(data_tokenized[i])-1:\n",
    "            neg_sent_tokens.append(data_tokenized[i][j])\n",
    "            # If current token is a auxiliary verb and the next token is not a variation of 'not' then add 'not'\n",
    "            if data_tokenized[i][j+1].lower() not in ['not', \"n't\"]: \n",
    "                neg_sent_tokens.append('not')\n",
    "                neg_flag = 0\n",
    "        else:\n",
    "            # Otherwise simply add the token as it is\n",
    "            neg_sent_tokens.append(data_tokenized[i][j])\n",
    "        if data_tokenized[i][j] in string.punctuation or sent_pos_tagged[j][1] in ['IN', 'CC'] or data_tokenized[i][j] in ['--', '..', '...', '....']:\n",
    "            # If a punctuation or conjuction appears in the sentence then most of the times there are two\n",
    "            #  or more parts so we will consider the part after it as an independent sentence and will allow it's \n",
    "            # negation\n",
    "            neg_flag = 1\n",
    "    \n",
    "    # Since, 's, etc are tokenized simply adding them separated by space changes the sentence, we deal with that below\n",
    "    # Even the punctuation marks are separated out by space if we simply use join. Hence, we process the spaces aptly\n",
    "    negated_sentence = neg_sent_tokens[0]\n",
    "    for token in neg_sent_tokens[1:]:\n",
    "        negated_sentence += token if token[0] in string.punctuation or token == \"n't\" else \" \" + token\n",
    "    data_negated.append(negated_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e63fad",
   "metadata": {},
   "source": [
    "The following table shows the original sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3f8cc408",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Conceptually cream skimming has two basic dime...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>you know during the season and i guess at at y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>One of our number will carry out your instruct...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How do you know? All this is their information...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>yeah i tell you what though if you go price so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>right because clothes are some are really expe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>I'll admit that it wasn't he who bought strych...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>Particularly noteworthy are the ornately Frenc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>Do you mean tall or short?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>Some writers hint that the only reason Ta Mok ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      0\n",
       "0     Conceptually cream skimming has two basic dime...\n",
       "1     you know during the season and i guess at at y...\n",
       "2     One of our number will carry out your instruct...\n",
       "3     How do you know? All this is their information...\n",
       "4     yeah i tell you what though if you go price so...\n",
       "...                                                 ...\n",
       "4995  right because clothes are some are really expe...\n",
       "4996  I'll admit that it wasn't he who bought strych...\n",
       "4997  Particularly noteworthy are the ornately Frenc...\n",
       "4998                         Do you mean tall or short?\n",
       "4999  Some writers hint that the only reason Ta Mok ...\n",
       "\n",
       "[5000 rows x 1 columns]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f9cfc8",
   "metadata": {},
   "source": [
    "The following are the negated sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "88101d35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Conceptually cream skimming has not two basic ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>you know during the season and i guess at at y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>One of our number will not carry out your inst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How do not you know? All this is not their inf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>yeah i tell you what though if you go price so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>right because clothes are not some are really ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>I'll not admit that it was he who bought stryc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>Particularly noteworthy are not the ornately F...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>Do not you mean tall or short?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>Some writers hint that the only reason Ta Mok ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      0\n",
       "0     Conceptually cream skimming has not two basic ...\n",
       "1     you know during the season and i guess at at y...\n",
       "2     One of our number will not carry out your inst...\n",
       "3     How do not you know? All this is not their inf...\n",
       "4     yeah i tell you what though if you go price so...\n",
       "...                                                 ...\n",
       "4995  right because clothes are not some are really ...\n",
       "4996  I'll not admit that it was he who bought stryc...\n",
       "4997  Particularly noteworthy are not the ornately F...\n",
       "4998                     Do not you mean tall or short?\n",
       "4999  Some writers hint that the only reason Ta Mok ...\n",
       "\n",
       "[5000 rows x 1 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(data_negated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f48c00d",
   "metadata": {},
   "source": [
    "Writing the results in a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "03a314e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"rishi_dey_chowdhury_negation.csv\"\n",
    "fields = ['index', 'real_sentence', 'fake_sentence'] \n",
    "with open(filename, 'w') as csvfile: \n",
    "    csvwriter = csv.writer(csvfile) \n",
    "    csvwriter.writerow(fields)\n",
    "    for i in range(len(data)):\n",
    "        csvwriter.writerow([i, data[i], data_negated[i]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
